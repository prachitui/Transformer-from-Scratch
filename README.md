# Transformers

Transformers are considered novel deep feed-forward artificial neural network architectures that leverage self-attention mechanisms and can handle long-range correlations between the 
input-sequence items. Thanks to their massive success in the industry and academic research, bountiful transformer architectures — a.k.a. X-formers — have been proposed by researchers since
their inception in 2017, and have been adopted in a substantial number of domains such as — initially was proposed for — natural language processing (NLP), computer 
vision (CV), audio and speech processing, chemistry, and life sciences; they can achieve SOTA performances in the disciplines mentioned earlier.

![screenshot](Xmer.png)

## Reading
[2017 Vaswani et al.] [Attention is all you Need](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)

### Description
 In the notebook, we dive into the details of attention mechanism and implement self-attention layer and build our own transformer model from scratch.
 
Happy Learning! :)
